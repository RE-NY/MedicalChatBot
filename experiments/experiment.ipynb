{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "import pinecone\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import CTransformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining some helper functions\n",
    "These will be transfered to utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to extract data from pdf(load the pdf)\n",
    "def load_pdf(data):                 #directory path as input\n",
    "    loader = DirectoryLoader(data,\n",
    "                             glob = \"*.pdf\", #glob specifies which type of files to include, we specified it using regex\n",
    "                             loader_cls = PyPDFLoader)\n",
    "    \n",
    "    document = loader.load()\n",
    "    return document\n",
    "\n",
    "## Function to create text chunks based on sementic and other similarites\n",
    "def text_split(loaded_data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size = 200, chunk_overlap = 20)\n",
    "    text_chunks = text_splitter.split_documents(loaded_data)\n",
    "\n",
    "    return text_chunks\n",
    "\n",
    "## Function to download the embedding model from huggingface\n",
    "def download_hugging_face_embeddings_model():\n",
    "    '''Downloads the all-MiniLM-L6-v2 embedding model from hugging face'''\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading and making chunks of input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data = load_pdf(\"data_source/\")\n",
    "\n",
    "text_chunks = text_split(extracted_data)\n",
    "print(\"length of my chunk:\", len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b04434af3994985a1e180b7ee696d9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9850bb21f6ec4539a3b3d82a8a5717db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "075477d02d8f4e6996dd3ed5bf43c766",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8710aeac141e459082a74bf2b4cfe59b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c1b08b0799248579c78a0361d72bfa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3689137d5fd946c7b5db4a98acaa8adf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff0d493fdda34c1c96423d6189d3cac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67a1fd414e7840acbf139b51e04e918b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf8868168cb345128e9168196b88c548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "698d639449114c099a609ea0a5eed4d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c92403d989c493d81b8a832e507c406",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       "), model_name='sentence-transformers/all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = download_hugging_face_embeddings_model()\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 384\n"
     ]
    }
   ],
   "source": [
    "query_result = embeddings.embed_query(\"Hello world\")\n",
    "print(\"Length : \", len(query_result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "Embedding looks like this :  [-0.03447727486491203, 0.031023181974887848, 0.006734966300427914] .....\n"
     ]
    }
   ],
   "source": [
    "print(type(query_result))\n",
    "print(\"Embedding looks like this : \", query_result[:3], \".....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY')\n",
    "PINECONE_API_ENV = os.environ.get('PINECONE_API_ENV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "pc = Pinecone(api_key=os.environ.get(\"PINECONE_API_KEY\"))\n",
    "\n",
    "index_name = \"medical-chatbot\"\n",
    "\n",
    "## If index with this name is not present then create it.\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1536, \n",
    "        metric=\"cosine\", \n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\"#, \n",
    "            #region=\"us-east-1\"\n",
    "        ) \n",
    "    ) \n",
    "\n",
    "\n",
    "## Create a \"namespace\" section inside the index for efficient search(see documentation)\n",
    "import time\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "namespace = \"wondervector5000\"\n",
    "\n",
    "## This captures all the embedding data and is later used to make retriever, for retrieval.\n",
    "docsearch = PineconeVectorStore.from_documents(\n",
    "    documents = text_chunks,\n",
    "    index_name = index_name,\n",
    "    embedding = embeddings, \n",
    "    namespace = namespace \n",
    ")\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "## Making of prompt template for passing the context and question to LLM.\n",
    "prompt_template=\"\"\"\n",
    "Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up any answer.\n",
    "\n",
    "The Context is : {context}\n",
    "The Question is : {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\"\n",
    "## Final prompt template\n",
    "medical_prompt_template=PromptTemplate(input_variables=[\"context\", \"question\"], template=prompt_template)\n",
    "\n",
    "\n",
    "## LLM used is loaded from model folder and is downloaded from huggingface as a bin file.\n",
    "##CTransformers library is used to load the model. You can use any model from huggingface/OpenAI etc. Here we have used\n",
    "## meta llama-2-7b model.\n",
    "llm=CTransformers(model=\"model/llama-2-7b-chat.ggmlv3.q4_0.bin\",\n",
    "                  model_type=\"llama\",\n",
    "                  config={'max_new_tokens':512,\n",
    "                          'temperature':0.75})\n",
    "\n",
    "\n",
    "chain_type_kwargs={\"prompt\": medical_prompt_template}\n",
    "\n",
    "\n",
    "## Initializing the retrievelQA chain. Notice the kwargs of retriever, the k value is used by k-nearest neighbour algorithm\n",
    "## for sementic search and returns 'k' most relavent chunks from the namespace of index relavent to the question.\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    chain_type_kwargs=chain_type_kwargs,\n",
    "    retriever=docsearch.as_retriever(search_kwargs={'k': 2}),\n",
    "    return_source_documents=True\n",
    "    )\n",
    "\n",
    "## qa will finally be used whenever user asks a question. So storing in index was a one-time effort, now we can use\n",
    "## the sementic search capability of vector stores to retrieve 'k' most relavent chunks and use them as context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## How to query from a namespace in a particular index\n",
    "pc = Pinecone(api_key='YOUR_API_KEY')\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "index.query(\n",
    "    namespace=\"Name of the namespace\",\n",
    "    vector=\"Vector formed by embeeding of qn.\",\n",
    "    top_k=3,\n",
    "    include_values=True\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
